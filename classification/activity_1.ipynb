{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "746335bf",
   "metadata": {},
   "source": [
    "## **Compare F1 Scores Across Algorithms**\n",
    "   - Evaluate and compare the F1 scores of the following classifiers: k-Nearest Neighbors (k-NN), Logistic Regression, Decision Tree, and Support Vector Machine (SVM).\n",
    "   - Use 10-fold cross-validation to obtain reliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0189e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "609799d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
      "0            0.00               0.64           0.64           0.0   \n",
      "1            0.21               0.28           0.50           0.0   \n",
      "2            0.06               0.00           0.71           0.0   \n",
      "3            0.00               0.00           0.00           0.0   \n",
      "4            0.00               0.00           0.00           0.0   \n",
      "\n",
      "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
      "0           0.32            0.00              0.00                0.00   \n",
      "1           0.14            0.28              0.21                0.07   \n",
      "2           1.23            0.19              0.19                0.12   \n",
      "3           0.63            0.00              0.31                0.63   \n",
      "4           0.63            0.00              0.31                0.63   \n",
      "\n",
      "   word_freq_order  word_freq_mail  ...  char_freq_%3B  char_freq_%28  \\\n",
      "0             0.00            0.00  ...           0.00          0.000   \n",
      "1             0.00            0.94  ...           0.00          0.132   \n",
      "2             0.64            0.25  ...           0.01          0.143   \n",
      "3             0.31            0.63  ...           0.00          0.137   \n",
      "4             0.31            0.63  ...           0.00          0.135   \n",
      "\n",
      "   char_freq_%5B  char_freq_%21  char_freq_%24  char_freq_%23  \\\n",
      "0            0.0          0.778          0.000          0.000   \n",
      "1            0.0          0.372          0.180          0.048   \n",
      "2            0.0          0.276          0.184          0.010   \n",
      "3            0.0          0.137          0.000          0.000   \n",
      "4            0.0          0.135          0.000          0.000   \n",
      "\n",
      "   capital_run_length_average  capital_run_length_longest  \\\n",
      "0                       3.756                          61   \n",
      "1                       5.114                         101   \n",
      "2                       9.821                         485   \n",
      "3                       3.537                          40   \n",
      "4                       3.537                          40   \n",
      "\n",
      "   capital_run_length_total  class  \n",
      "0                       278      1  \n",
      "1                      1028      1  \n",
      "2                      2259      1  \n",
      "3                       191      1  \n",
      "4                       191      1  \n",
      "\n",
      "[5 rows x 58 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import dataset\n",
    "df = pd.read_csv('spambase.csv')\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af4491b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to split the dataset into training and testing sets\n",
    "# We will use 40% of the dataset for testing and 60% for training\n",
    "# We will use a random state of 0 to ensure reproducibility\n",
    "# We will use a stratified split to ensure that the training and testing sets have the same distribution of the target variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c579a92a",
   "metadata": {},
   "source": [
    "## k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee1350cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84      1097\n",
      "           1       0.78      0.70      0.74       744\n",
      "\n",
      "    accuracy                           0.80      1841\n",
      "   macro avg       0.79      0.78      0.79      1841\n",
      "weighted avg       0.80      0.80      0.80      1841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "cls = KNeighborsClassifier(n_neighbors=5)\n",
    "cls.fit(X_train, y_train)\n",
    "\n",
    "pred = cls.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af0bd138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 0. ],\n",
       "       [1. , 0. ],\n",
       "       [0.4, 0.6],\n",
       "       [0. , 1. ],\n",
       "       [1. , 0. ],\n",
       "       [1. , 0. ],\n",
       "       [0.8, 0.2],\n",
       "       [0. , 1. ],\n",
       "       [1. , 0. ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.predict_proba(X_test.iloc[1:10,]).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809217af",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3dd54f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93      1097\n",
      "           1       0.92      0.87      0.89       744\n",
      "\n",
      "    accuracy                           0.92      1841\n",
      "   macro avg       0.92      0.91      0.91      1841\n",
      "weighted avg       0.92      0.92      0.91      1841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cls = LogisticRegression(random_state=21, max_iter=3000)\n",
    "cls.fit(X_train, y_train)\n",
    "\n",
    "pred = cls.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4be5003",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d52f6254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92      1097\n",
      "           1       0.87      0.89      0.88       744\n",
      "\n",
      "    accuracy                           0.90      1841\n",
      "   macro avg       0.90      0.90      0.90      1841\n",
      "weighted avg       0.90      0.90      0.90      1841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "cls = DecisionTreeClassifier(random_state=0)\n",
    "cls.fit(X_train, y_train)\n",
    "\n",
    "pred = cls.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb1b714",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1674bae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.90      0.78      1097\n",
      "           1       0.73      0.42      0.53       744\n",
      "\n",
      "    accuracy                           0.70      1841\n",
      "   macro avg       0.71      0.66      0.66      1841\n",
      "weighted avg       0.71      0.70      0.68      1841\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "cls = SVC(random_state=0, probability=True)\n",
    "cls.fit(X_train, y_train)\n",
    "\n",
    "pred = cls.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388ee3ad",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b5c682",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "717f4c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier() 0.7627129430489389\n",
      "LogisticRegression(max_iter=3000) 0.9273957017957528\n",
      "DecisionTreeClassifier() 0.9012907438359076\n",
      "SVC(probability=True, random_state=0) 0.657281825149173\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "lr = LogisticRegression(max_iter=3000)\n",
    "dt = DecisionTreeClassifier()\n",
    "svm = SVC(random_state=0, probability=True)\n",
    "\n",
    "for cls in [knn, lr, dt, svm]:\n",
    "  scores = cross_val_score(cls, X_train, y_train, cv=10, scoring='f1_macro')\n",
    "  print(cls, scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626ef28",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52f6be1",
   "metadata": {},
   "source": [
    "We observed that without any hyperparameter tunning the Logistic Regression was the algorithm that has the most F1. \n",
    "The SVM was the worst, given its nature that almost always requires a hyperparameter tunning to give reliable results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
